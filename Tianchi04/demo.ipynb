{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f012208b",
   "metadata": {
    "id": "f012208b"
   },
   "source": [
    "# Sparse-Adversarial-Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dbb59d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68dbb59d",
    "outputId": "4392ec87-b599-4104-a11e-bb6c54225104"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fzh27ICUmmvF",
   "metadata": {
    "id": "fzh27ICUmmvF"
   },
   "outputs": [],
   "source": [
    "!rm -rf ../images/.ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27534e00-2c59-418c-a877-cc763d5dc78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/ae-attack-pipeline/Tianchi04\n"
     ]
    }
   ],
   "source": [
    "%cd Tianchi04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W0VuNwp6l8Ys",
   "metadata": {
    "id": "W0VuNwp6l8Ys"
   },
   "outputs": [],
   "source": [
    "!python attack_3.py --max_iter=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BHWoXHewVunn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "BHWoXHewVunn",
    "outputId": "6a87fb0f-25c0-4079-bc33-ae11ebaa2c0d"
   },
   "outputs": [],
   "source": [
    "# Copy and move newly generated files from 'final' folder to 'select1000_new_p'\n",
    "import os\n",
    "\n",
    "final_files = os.listdir(\"./final\")\n",
    "final_files.remove(\".ipynb_checkpoints\")\n",
    "\n",
    "for file in final_files:\n",
    "    file_path = \"final/\" + file\n",
    "    out_dir = \"select1000_new_p/\" + file\n",
    "    %cp -av \"{file_path}\" \"{out_dir}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bOOUDEPulIki",
   "metadata": {
    "id": "bOOUDEPulIki"
   },
   "outputs": [],
   "source": [
    "# Run when eval.py gets following error:\n",
    "#   IsADirectoryError: [Errno 21] Is a directory: './select1000_new/.ipynb_checkpoints'\n",
    "!rm -rf ./select1000_new_p/.ipynb_checkpoints/\n",
    "!rm -rf ./select1000_new/.ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JCC5iJJuYVrO",
   "metadata": {
    "id": "JCC5iJJuYVrO"
   },
   "outputs": [],
   "source": [
    "!python eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc48b09",
   "metadata": {
    "id": "afc48b09"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "UcZjREM9eDxt",
   "metadata": {
    "id": "UcZjREM9eDxt"
   },
   "source": [
    "# Faster-RCNN test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z9lYB_pleF_W",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Z9lYB_pleF_W",
    "outputId": "bcec1070-112e-4631-e9b0-f94a6d7fe4f9"
   },
   "outputs": [],
   "source": [
    "from mmdet.apis import inference_detector, init_detector, show_result_pyplot\n",
    "import os\n",
    "import mmcv\n",
    "import numpy as np\n",
    "\n",
    "images = os.listdir('select1000_new/')\n",
    "#images = ['1.jpg']\n",
    "if '.ipynb_checkpoints' in images:\n",
    "  # Removing common error\n",
    "  images.remove('.ipynb_checkpoints')\n",
    "\n",
    "# Choose to use a config and initialize the detector\n",
    "config = './mmdetection/configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\n",
    "# Setup a checkpoint file to load\n",
    "checkpoint = './checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\n",
    "# initialize the detector\n",
    "model = init_detector(config, checkpoint, device='cuda:0')\n",
    "\n",
    "for image in images:\n",
    "    # Use the detector to do inference\n",
    "    img0 = 'select1000_new/'+image     # Benign\n",
    "    img1 = 'select1000_new_p/'+image   # Adversarial\n",
    "    result0 = inference_detector(model, img0)\n",
    "    result1 = inference_detector(model, img1)\n",
    "    #meta = [{'filename': '../images/6.png', 'ori_filename': '../images/6.png', 'ori_shape': (500, 500, 3), 'img_shape': (800, 800, 3), 'pad_shape': (800, 800, 3), 'scale_factor': np.array([1.6, 1.6, 1.6, 1.6], dtype=np.float32), 'flip': False, 'flip_direction': None, 'img_norm_cfg': {'mean': np.array([123.675, 116.28 , 103.53 ], dtype=np.float32), 'std': np.array([58.395, 57.12 , 57.375], dtype=np.float32), 'to_rgb': True}}]\n",
    "    #result1, label = model(return_loss=False, rescale=True, img=img1, img_metas=meta)\n",
    "\n",
    "    show_result_pyplot(model, img0, result0, score_thr=0.5, title=\"Benign\")\n",
    "    show_result_pyplot(model, img1, result1, score_thr=0.5, title=\"Adversarial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FCc51hV-jGF9",
   "metadata": {
    "id": "FCc51hV-jGF9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "TiK8cOBrkGRc",
   "metadata": {
    "id": "TiK8cOBrkGRc"
   },
   "source": [
    "# YOLOv4 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "GihMZ-GrnJjF",
   "metadata": {
    "id": "GihMZ-GrnJjF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3.jpeg']\n",
      "convolution havn't activate linear\n",
      "convolution havn't activate linear\n",
      "convolution havn't activate linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxes1\n",
      "[[0.7503393308112496, 0.48561922735289526, 0.3392335513704701, 0.2918040536736187, 0.97512984, 0.9990301, 2], [0.9651694736982647, 0.4836887191785009, 0.0685895726476845, 0.23051454440543526, 0.9650091, 0.9999683, 2], [0.45627671166470174, 0.4565121186406989, 0.2657462451606989, 0.503198552582609, 0.95275486, 0.99999607, 0], [0.09180355699438798, 0.39072853326797485, 0.18494939137446254, 0.4892964072917637, 0.89397335, 0.9978108, 7], [0.34025577965535614, 0.3241395816991204, 0.2760306545778325, 0.450896259985472, 0.74515593, 0.9996319, 7]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from utils.utils import *\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from tool.darknet2pytorch import *\n",
    "from infer import infer\n",
    "from tqdm import tqdm\n",
    "from skimage import measure\n",
    "\n",
    "\n",
    "selected_path = './select1000_new_p'\n",
    "files = os.listdir(selected_path)\n",
    "files.sort()\n",
    "files = [\"3.jpeg\"]\n",
    "print(files)\n",
    "\n",
    "cfgfile = \"checkpoints/yolov4.cfg\"\n",
    "weightfile = \"checkpoints/yolov4.weights\"\n",
    "darknet_model = Darknet(cfgfile)\n",
    "darknet_model.load_weights(weightfile)\n",
    "darknet_model = darknet_model.eval().cuda()\n",
    "\n",
    "for img_name_index in tqdm(range(len(files))):\n",
    "  img_name = files[img_name_index]\n",
    "\n",
    "  img_path1 = os.path.join(selected_path, img_name)\n",
    "  img1 = Image.open(img_path1).convert('RGB')\n",
    "\n",
    "\n",
    "  resize_small = transforms.Compose([\n",
    "      transforms.Resize((608, 608)),\n",
    "  ])\n",
    "  #img0 = resize_small(img0)\n",
    "  img1 = resize_small(img1)\n",
    "\n",
    "  # --------------------BOX score\n",
    "  boxes1 = do_detect(darknet_model, img1, 0.5, 0.4, True)\n",
    "  print(\"boxes1\")\n",
    "  print(boxes1)\n",
    "\n",
    "# TODO: Plot the YOLOv4 detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82986bb-0939-40a5-a8f4-123d2626e94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "model1 = Yolov4(yolov4conv137weight=None, n_classes=80, inference=True)\n",
    "pretrained_dict = torch.load('checkpoints/yolov4.pth', map_location=torch.device('cuda'))\n",
    "model1.load_state_dict(pretrained_dict)\n",
    "model1.eval().cuda()\n",
    "\n",
    "img_pil = cv2.imread('select1000_new/1.jpg')\n",
    "img_pil = cv2.cvtColor(img_pil, cv2.COLOR_BGR2RGB)\n",
    "img_pil = np.transpose(img_pil, (2,0,1))\n",
    "img = torch.from_numpy(img_pil/255.).float()\n",
    "img = img.unsqueeze(0).cuda()\n",
    "\n",
    "image = F.interpolate(img, size=(608, 608), mode='bilinear', align_corners=True)\n",
    "out1 = model1(image)\n",
    "print(out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad66439-39b4-439d-83ca-7fb7cf89f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tool.utils import *\n",
    "from tool.torch_utils import do_detect\n",
    "from models import *\n",
    "\n",
    "model = Yolov4(yolov4conv137weight=None, n_classes=80, inference=True)\n",
    "pretrained_dict = torch.load('checkpoints/yolov4.pth', map_location=torch.device('cuda'))\n",
    "model.load_state_dict(pretrained_dict)\n",
    "model.eval().cuda()\n",
    "\n",
    "image_name = \"1.jpg\"\n",
    "#image_name = \"2.png\"\n",
    "#image_name = \"3.jpeg\"\n",
    "image = \"select1000_new_p/\" + image_name\n",
    "width = 512\n",
    "height = 512\n",
    "\n",
    "img = cv2.imread(image)\n",
    "\n",
    "# sized = cv2.resize(img, (width, height))\n",
    "# sized = cv2.cvtColor(sized, cv2.COLOR_BGR2RGB)\n",
    "# boxes = do_detect(model, sized, 0.4, 0.6, True) \n",
    "\n",
    "boxes = do_detect(model, img, 0.4, 0.6, True) \n",
    "\n",
    "namesfile = 'data/coco.names'\n",
    "class_names = load_class_names(namesfile)\n",
    "print(boxes)\n",
    "result = plot_boxes_cv2(img, boxes[0], 'predictions/{}'.format(image_name), class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef6a755-7bf1-4986-99e7-26332b696d55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "SAA-demo.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-9.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
